{"ast":null,"code":"var _s = $RefreshSig$();\nimport { useState, useRef, useCallback } from 'react';\nexport const useVoiceAnalysis = () => {\n  _s();\n  const [isAnalyzing, setIsAnalyzing] = useState(false);\n  const [results, setResults] = useState(null);\n  const audioContextRef = useRef(null);\n  const analyserRef = useRef(null);\n  const microphoneRef = useRef(null);\n  const recognitionRef = useRef(null);\n  const startVoiceAnalysis = useCallback(() => {\n    setIsAnalyzing(true);\n\n    // Initialize speech recognition\n    if ('webkitSpeechRecognition' in window || 'SpeechRecognition' in window) {\n      const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;\n      recognitionRef.current = new SpeechRecognition();\n      recognitionRef.current.continuous = true;\n      recognitionRef.current.interimResults = true;\n      recognitionRef.current.lang = 'en-US';\n      let transcript = '';\n      let wordCount = 0;\n      let fillerWordCount = 0;\n      const startTime = Date.now();\n      recognitionRef.current.onresult = event => {\n        for (let i = event.resultIndex; i < event.results.length; i++) {\n          const result = event.results[i];\n          if (result.isFinal) {\n            transcript += result[0].transcript;\n            const words = result[0].transcript.split(' ');\n            wordCount += words.length;\n\n            // Count filler words\n            const fillerWords = ['um', 'uh', 'like', 'you know', 'so', 'well', 'actually'];\n            words.forEach(word => {\n              if (fillerWords.includes(word.toLowerCase().replace(/[^\\w]/g, ''))) {\n                fillerWordCount++;\n              }\n            });\n          }\n        }\n      };\n      recognitionRef.current.onend = () => {\n        const duration = (Date.now() - startTime) / 1000 / 60; // minutes\n        const speakingRate = wordCount / duration;\n        setResults({\n          fillerWords: fillerWordCount,\n          speakingRate: Math.round(speakingRate),\n          volume: 75,\n          // Mock data - would need audio analysis\n          clarity: 88,\n          // Mock data - would need audio analysis\n          transcript\n        });\n        setIsAnalyzing(false);\n      };\n      recognitionRef.current.start();\n    }\n\n    // Initialize audio analysis for volume\n    navigator.mediaDevices.getUserMedia({\n      audio: true\n    }).then(stream => {\n      audioContextRef.current = new AudioContext();\n      analyserRef.current = audioContextRef.current.createAnalyser();\n      microphoneRef.current = audioContextRef.current.createMediaStreamSource(stream);\n      microphoneRef.current.connect(analyserRef.current);\n      analyserRef.current.fftSize = 256;\n\n      // Start volume monitoring\n      monitorVolume();\n    }).catch(error => {\n      console.error('Error accessing microphone:', error);\n    });\n  }, []);\n  const monitorVolume = useCallback(() => {\n    if (!analyserRef.current) return;\n    const dataArray = new Uint8Array(analyserRef.current.frequencyBinCount);\n    analyserRef.current.getByteFrequencyData(dataArray);\n\n    // You could use this for real-time volume monitoring\n    // const average = dataArray.reduce((a, b) => a + b) / dataArray.length;\n\n    requestAnimationFrame(monitorVolume);\n  }, []);\n  const stopVoiceAnalysis = useCallback(() => {\n    if (recognitionRef.current) {\n      recognitionRef.current.stop();\n    }\n    if (audioContextRef.current) {\n      audioContextRef.current.close();\n    }\n    setIsAnalyzing(false);\n  }, []);\n  return {\n    startVoiceAnalysis,\n    stopVoiceAnalysis,\n    isAnalyzing,\n    voiceResults: results\n  };\n};\n_s(useVoiceAnalysis, \"NHxWVoXWXYwNiBa8ZqM66/jUnNQ=\");","map":{"version":3,"names":["useState","useRef","useCallback","useVoiceAnalysis","_s","isAnalyzing","setIsAnalyzing","results","setResults","audioContextRef","analyserRef","microphoneRef","recognitionRef","startVoiceAnalysis","window","SpeechRecognition","webkitSpeechRecognition","current","continuous","interimResults","lang","transcript","wordCount","fillerWordCount","startTime","Date","now","onresult","event","i","resultIndex","length","result","isFinal","words","split","fillerWords","forEach","word","includes","toLowerCase","replace","onend","duration","speakingRate","Math","round","volume","clarity","start","navigator","mediaDevices","getUserMedia","audio","then","stream","AudioContext","createAnalyser","createMediaStreamSource","connect","fftSize","monitorVolume","catch","error","console","dataArray","Uint8Array","frequencyBinCount","getByteFrequencyData","requestAnimationFrame","stopVoiceAnalysis","stop","close","voiceResults"],"sources":["/Users/irisxu/Documents/DubHacks2025/dubhacksmain/src/hooks/useVoiceAnalysis.ts"],"sourcesContent":["import { useState, useRef, useCallback } from 'react';\n\ninterface VoiceAnalysisResults {\n  fillerWords: number;\n  speakingRate: number;\n  volume: number;\n  clarity: number;\n  transcript: string;\n}\n\nexport const useVoiceAnalysis = () => {\n  const [isAnalyzing, setIsAnalyzing] = useState(false);\n  const [results, setResults] = useState<VoiceAnalysisResults | null>(null);\n  const audioContextRef = useRef<AudioContext | null>(null);\n  const analyserRef = useRef<AnalyserNode | null>(null);\n  const microphoneRef = useRef<MediaStreamAudioSourceNode | null>(null);\n  const recognitionRef = useRef<SpeechRecognition | null>(null);\n\n  const startVoiceAnalysis = useCallback(() => {\n    setIsAnalyzing(true);\n    \n    // Initialize speech recognition\n    if ('webkitSpeechRecognition' in window || 'SpeechRecognition' in window) {\n      const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;\n      recognitionRef.current = new SpeechRecognition();\n      \n      recognitionRef.current.continuous = true;\n      recognitionRef.current.interimResults = true;\n      recognitionRef.current.lang = 'en-US';\n\n      let transcript = '';\n      let wordCount = 0;\n      let fillerWordCount = 0;\n      const startTime = Date.now();\n\n      recognitionRef.current.onresult = (event: SpeechRecognitionEvent) => {\n        for (let i = event.resultIndex; i < event.results.length; i++) {\n          const result = event.results[i];\n          if (result.isFinal) {\n            transcript += result[0].transcript;\n            const words = result[0].transcript.split(' ');\n            wordCount += words.length;\n            \n            // Count filler words\n            const fillerWords = ['um', 'uh', 'like', 'you know', 'so', 'well', 'actually'];\n            words.forEach((word: string) => {\n              if (fillerWords.includes(word.toLowerCase().replace(/[^\\w]/g, ''))) {\n                fillerWordCount++;\n              }\n            });\n          }\n        }\n      };\n\n      recognitionRef.current.onend = () => {\n        const duration = (Date.now() - startTime) / 1000 / 60; // minutes\n        const speakingRate = wordCount / duration;\n        \n        setResults({\n          fillerWords: fillerWordCount,\n          speakingRate: Math.round(speakingRate),\n          volume: 75, // Mock data - would need audio analysis\n          clarity: 88, // Mock data - would need audio analysis\n          transcript\n        });\n        setIsAnalyzing(false);\n      };\n\n      recognitionRef.current.start();\n    }\n\n    // Initialize audio analysis for volume\n    navigator.mediaDevices.getUserMedia({ audio: true })\n      .then(stream => {\n        audioContextRef.current = new AudioContext();\n        analyserRef.current = audioContextRef.current.createAnalyser();\n        microphoneRef.current = audioContextRef.current.createMediaStreamSource(stream);\n        \n        microphoneRef.current.connect(analyserRef.current);\n        analyserRef.current.fftSize = 256;\n        \n        // Start volume monitoring\n        monitorVolume();\n      })\n      .catch(error => {\n        console.error('Error accessing microphone:', error);\n      });\n  }, []);\n\n  const monitorVolume = useCallback(() => {\n    if (!analyserRef.current) return;\n    \n    const dataArray = new Uint8Array(analyserRef.current.frequencyBinCount);\n    analyserRef.current.getByteFrequencyData(dataArray);\n    \n    // You could use this for real-time volume monitoring\n    // const average = dataArray.reduce((a, b) => a + b) / dataArray.length;\n    \n    requestAnimationFrame(monitorVolume);\n  }, []);\n\n  const stopVoiceAnalysis = useCallback(() => {\n    if (recognitionRef.current) {\n      recognitionRef.current.stop();\n    }\n    \n    if (audioContextRef.current) {\n      audioContextRef.current.close();\n    }\n    \n    setIsAnalyzing(false);\n  }, []);\n\n  return {\n    startVoiceAnalysis,\n    stopVoiceAnalysis,\n    isAnalyzing,\n    voiceResults: results\n  };\n};\n"],"mappings":";AAAA,SAASA,QAAQ,EAAEC,MAAM,EAAEC,WAAW,QAAQ,OAAO;AAUrD,OAAO,MAAMC,gBAAgB,GAAGA,CAAA,KAAM;EAAAC,EAAA;EACpC,MAAM,CAACC,WAAW,EAAEC,cAAc,CAAC,GAAGN,QAAQ,CAAC,KAAK,CAAC;EACrD,MAAM,CAACO,OAAO,EAAEC,UAAU,CAAC,GAAGR,QAAQ,CAA8B,IAAI,CAAC;EACzE,MAAMS,eAAe,GAAGR,MAAM,CAAsB,IAAI,CAAC;EACzD,MAAMS,WAAW,GAAGT,MAAM,CAAsB,IAAI,CAAC;EACrD,MAAMU,aAAa,GAAGV,MAAM,CAAoC,IAAI,CAAC;EACrE,MAAMW,cAAc,GAAGX,MAAM,CAA2B,IAAI,CAAC;EAE7D,MAAMY,kBAAkB,GAAGX,WAAW,CAAC,MAAM;IAC3CI,cAAc,CAAC,IAAI,CAAC;;IAEpB;IACA,IAAI,yBAAyB,IAAIQ,MAAM,IAAI,mBAAmB,IAAIA,MAAM,EAAE;MACxE,MAAMC,iBAAiB,GAAGD,MAAM,CAACC,iBAAiB,IAAID,MAAM,CAACE,uBAAuB;MACpFJ,cAAc,CAACK,OAAO,GAAG,IAAIF,iBAAiB,CAAC,CAAC;MAEhDH,cAAc,CAACK,OAAO,CAACC,UAAU,GAAG,IAAI;MACxCN,cAAc,CAACK,OAAO,CAACE,cAAc,GAAG,IAAI;MAC5CP,cAAc,CAACK,OAAO,CAACG,IAAI,GAAG,OAAO;MAErC,IAAIC,UAAU,GAAG,EAAE;MACnB,IAAIC,SAAS,GAAG,CAAC;MACjB,IAAIC,eAAe,GAAG,CAAC;MACvB,MAAMC,SAAS,GAAGC,IAAI,CAACC,GAAG,CAAC,CAAC;MAE5Bd,cAAc,CAACK,OAAO,CAACU,QAAQ,GAAIC,KAA6B,IAAK;QACnE,KAAK,IAAIC,CAAC,GAAGD,KAAK,CAACE,WAAW,EAAED,CAAC,GAAGD,KAAK,CAACrB,OAAO,CAACwB,MAAM,EAAEF,CAAC,EAAE,EAAE;UAC7D,MAAMG,MAAM,GAAGJ,KAAK,CAACrB,OAAO,CAACsB,CAAC,CAAC;UAC/B,IAAIG,MAAM,CAACC,OAAO,EAAE;YAClBZ,UAAU,IAAIW,MAAM,CAAC,CAAC,CAAC,CAACX,UAAU;YAClC,MAAMa,KAAK,GAAGF,MAAM,CAAC,CAAC,CAAC,CAACX,UAAU,CAACc,KAAK,CAAC,GAAG,CAAC;YAC7Cb,SAAS,IAAIY,KAAK,CAACH,MAAM;;YAEzB;YACA,MAAMK,WAAW,GAAG,CAAC,IAAI,EAAE,IAAI,EAAE,MAAM,EAAE,UAAU,EAAE,IAAI,EAAE,MAAM,EAAE,UAAU,CAAC;YAC9EF,KAAK,CAACG,OAAO,CAAEC,IAAY,IAAK;cAC9B,IAAIF,WAAW,CAACG,QAAQ,CAACD,IAAI,CAACE,WAAW,CAAC,CAAC,CAACC,OAAO,CAAC,QAAQ,EAAE,EAAE,CAAC,CAAC,EAAE;gBAClElB,eAAe,EAAE;cACnB;YACF,CAAC,CAAC;UACJ;QACF;MACF,CAAC;MAEDX,cAAc,CAACK,OAAO,CAACyB,KAAK,GAAG,MAAM;QACnC,MAAMC,QAAQ,GAAG,CAAClB,IAAI,CAACC,GAAG,CAAC,CAAC,GAAGF,SAAS,IAAI,IAAI,GAAG,EAAE,CAAC,CAAC;QACvD,MAAMoB,YAAY,GAAGtB,SAAS,GAAGqB,QAAQ;QAEzCnC,UAAU,CAAC;UACT4B,WAAW,EAAEb,eAAe;UAC5BqB,YAAY,EAAEC,IAAI,CAACC,KAAK,CAACF,YAAY,CAAC;UACtCG,MAAM,EAAE,EAAE;UAAE;UACZC,OAAO,EAAE,EAAE;UAAE;UACb3B;QACF,CAAC,CAAC;QACFf,cAAc,CAAC,KAAK,CAAC;MACvB,CAAC;MAEDM,cAAc,CAACK,OAAO,CAACgC,KAAK,CAAC,CAAC;IAChC;;IAEA;IACAC,SAAS,CAACC,YAAY,CAACC,YAAY,CAAC;MAAEC,KAAK,EAAE;IAAK,CAAC,CAAC,CACjDC,IAAI,CAACC,MAAM,IAAI;MACd9C,eAAe,CAACQ,OAAO,GAAG,IAAIuC,YAAY,CAAC,CAAC;MAC5C9C,WAAW,CAACO,OAAO,GAAGR,eAAe,CAACQ,OAAO,CAACwC,cAAc,CAAC,CAAC;MAC9D9C,aAAa,CAACM,OAAO,GAAGR,eAAe,CAACQ,OAAO,CAACyC,uBAAuB,CAACH,MAAM,CAAC;MAE/E5C,aAAa,CAACM,OAAO,CAAC0C,OAAO,CAACjD,WAAW,CAACO,OAAO,CAAC;MAClDP,WAAW,CAACO,OAAO,CAAC2C,OAAO,GAAG,GAAG;;MAEjC;MACAC,aAAa,CAAC,CAAC;IACjB,CAAC,CAAC,CACDC,KAAK,CAACC,KAAK,IAAI;MACdC,OAAO,CAACD,KAAK,CAAC,6BAA6B,EAAEA,KAAK,CAAC;IACrD,CAAC,CAAC;EACN,CAAC,EAAE,EAAE,CAAC;EAEN,MAAMF,aAAa,GAAG3D,WAAW,CAAC,MAAM;IACtC,IAAI,CAACQ,WAAW,CAACO,OAAO,EAAE;IAE1B,MAAMgD,SAAS,GAAG,IAAIC,UAAU,CAACxD,WAAW,CAACO,OAAO,CAACkD,iBAAiB,CAAC;IACvEzD,WAAW,CAACO,OAAO,CAACmD,oBAAoB,CAACH,SAAS,CAAC;;IAEnD;IACA;;IAEAI,qBAAqB,CAACR,aAAa,CAAC;EACtC,CAAC,EAAE,EAAE,CAAC;EAEN,MAAMS,iBAAiB,GAAGpE,WAAW,CAAC,MAAM;IAC1C,IAAIU,cAAc,CAACK,OAAO,EAAE;MAC1BL,cAAc,CAACK,OAAO,CAACsD,IAAI,CAAC,CAAC;IAC/B;IAEA,IAAI9D,eAAe,CAACQ,OAAO,EAAE;MAC3BR,eAAe,CAACQ,OAAO,CAACuD,KAAK,CAAC,CAAC;IACjC;IAEAlE,cAAc,CAAC,KAAK,CAAC;EACvB,CAAC,EAAE,EAAE,CAAC;EAEN,OAAO;IACLO,kBAAkB;IAClByD,iBAAiB;IACjBjE,WAAW;IACXoE,YAAY,EAAElE;EAChB,CAAC;AACH,CAAC;AAACH,EAAA,CA7GWD,gBAAgB","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}